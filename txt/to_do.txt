add FLOAT versions for tensor operations in place especially
make a num, then delete
also for all the operation backprop check for (if oeprand_A != nullptr && operand_A->requires_grad == true)



0: transformer blocks, tests for blocks, save load. call the transofrmer "transformer_model.hpp"
1: dataloader and optimizer (standard and adam)
2: CUDA tutorial 
3: CUDA tensor ops



unrelated:
make a razor extension the underclocks cpu and sets settings to save power

